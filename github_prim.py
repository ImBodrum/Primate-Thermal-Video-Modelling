# -*- coding: utf-8 -*-
"""Github_Prim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LpikoRXTqrqvTamJiGeOlcSTErAH6-bQ
"""

#link google drive
from google.colab import drive
drive.mount('/content/drive')

#Imports
import numpy as np
import os
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
import tensorflow as tf
import torch
from skimage import io, filters
from PIL import Image
from google.colab import files
import pandas as pd
import math
import shutil
from IPython.display import display, clear_output
import time
from PIL import Image

# Canny Edge Detection
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load the thermal image from the TIFF file
tif_file = '/content/drive/MyDrive/Primate Project/FLIRV1.tif'  # Replace with your TIFF file path
thermal_image = cv2.imread(tif_file, cv2.IMREAD_UNCHANGED)

# Check if the image loaded correctly
if thermal_image is None:
    raise ValueError("Failed to load the TIFF file. Please check the file path and format.")

# Ensure the thermal image is treated as a 2D grayscale image
if len(thermal_image.shape) > 2:
    thermal_image = thermal_image[:, :, 0]

# Convert to float32 for better precision in further processing
gray_img = thermal_image.astype(np.float32)

# Apply Gaussian blur to the image to reduce noise
img_blur = cv2.GaussianBlur(gray_img, (5, 5), 0)

# Define lower and upper thresholds for the Canny detector
lower_th = 50
upper_th = 100

# Convert the image back to 8-bit before applying Canny
img_blur_8bit = cv2.convertScaleAbs(img_blur)
canny_image = cv2.Canny(img_blur_8bit, lower_th, upper_th)

# Display the original blurred image and the Canny edge detection result
fig, axs = plt.subplots(1, 2, figsize=(15, 5))

# Display the blurred image (for reference)
axs[0].imshow(img_blur, cmap='gray')
axs[0].axis('on')
axs[0].set_title("Blurred Thermal Image")

# Display the Canny edge detection result
axs[1].imshow(canny_image, cmap='gray')
axs[1].axis('off')
axs[1].set_title("Canny Edge Detection")

plt.show()

# Sobel Edge Detection
import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import convolve2d

# Load the thermal image from the TIFF file
tif_file = '/content/drive/MyDrive/Primate Project/FLIRV1.tif'  # Replace with your TIFF file path
thermal_image = cv2.imread(tif_file, cv2.IMREAD_UNCHANGED)

# Check if the image loaded correctly
if thermal_image is None:
    raise ValueError("Failed to load the TIFF file. Please check the file path and format.")

# Ensure the thermal image is treated as a 2D grayscale image
if len(thermal_image.shape) > 2:
    thermal_image = thermal_image[:, :, 0]

# Convert to float32 for better precision in further processing
gray_img = thermal_image.astype(np.float32)

# Apply Gaussian blur to the image to reduce noise
img_blur = cv2.GaussianBlur(gray_img, (5, 5), 0)

# Define Sobel kernels for horizontal and vertical edge detection
horizontal_kernel = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])
vertical_kernel = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])

# Apply the Sobel edge detection using the defined kernels
conv_horizontal = convolve2d(img_blur, horizontal_kernel, mode='same', boundary='symm')
conv_vertical = convolve2d(img_blur, vertical_kernel, mode='same', boundary='symm')

# Calculate the magnitude of the edges to obtain edge strength
mag_edges = np.sqrt(conv_horizontal**2 + conv_vertical**2)

# Normalize the magnitude to be between 0 and 255 for better visualization
norm_mag_edges = (255.0 * mag_edges / np.max(mag_edges)).astype(np.uint8)

# Display the original blurred image and the Sobel edge detection result
fig, axs = plt.subplots(1, 2, figsize=(15, 5))

# Display the blurred image (for reference)
axs[0].imshow(img_blur, cmap='gray')
axs[0].axis('on')
axs[0].set_title("Blurred Thermal Image")

# Display the Sobel edge detection result
axs[1].imshow(norm_mag_edges, cmap='gray')
axs[1].axis('on')
axs[1].set_title("Sobel Edge Detection")

plt.show()

# Binary fence mask + Hough + Dilation + post processing CLAHE
import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.restoration import denoise_tv_chambolle

# Load and normalize the thermal image
tif_file1 = '/content/drive/MyDrive/Primate Project/FLIR0993.tif'
thermal_image1 = cv2.imread(tif_file1, cv2.IMREAD_ANYDEPTH)
if thermal_image1 is None:
    raise FileNotFoundError(f"File {tif_file1} not found.")
thermal_image_normalized = cv2.normalize(thermal_image1, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)

# Define intensity range and create binary mask
fence_min = 40
fence_max = 90
binary_fence_mask = cv2.inRange(thermal_image_normalized, fence_min, fence_max)

# Hough Line Transform for fence detection
lines = cv2.HoughLinesP(
    binary_fence_mask,
    rho=1,
    theta=np.pi / 180,
    threshold=80,  # Higher threshold for prominent lines
    minLineLength=30,
    maxLineGap=10   # Smaller gap for better continuity
)
hough_line_mask = np.zeros_like(binary_fence_mask)
if lines is not None:
    for line in lines:
        x1, y1, x2, y2 = line[0]
        cv2.line(hough_line_mask, (x1, y1), (x2, y2), 255, thickness=2)

# Combine binary and Hough masks
combined_mask = cv2.bitwise_or(binary_fence_mask, hough_line_mask)

# Slightly reduced kernel size and fewer iterations
kernel = np.ones((3, 3), np.uint8)
dilated_mask = cv2.dilate(combined_mask, kernel, iterations=2)

# Multi-pass inpainting
# Pass 1: Remove prominent fence lines
inpainted_image_pass1 = cv2.inpaint(thermal_image_normalized, dilated_mask, inpaintRadius=15, flags=cv2.INPAINT_TELEA)

# Pass 2: Remove smaller residual lines
inpainted_image_pass2 = cv2.inpaint(inpainted_image_pass1, binary_fence_mask, inpaintRadius=10, flags=cv2.INPAINT_TELEA)

# Final pass: Additional refinement
inpainted_image_final = cv2.inpaint(inpainted_image_pass2, combined_mask, inpaintRadius=7, flags=cv2.INPAINT_TELEA)

# Ensure the final_filtered_image is scaled to 0-255 and converted to uint8
final_filtered_image_scaled = cv2.normalize(final_filtered_image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)

# Apply CLAHE for contrast enhancement
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
clahe_enhanced_image = clahe.apply(final_filtered_image_scaled)

# Visualization
fig, axs = plt.subplots(1, 7, figsize=(30, 8))

axs[0].imshow(thermal_image_normalized, cmap='gray')
axs[0].axis('off')
axs[0].set_title("Original Thermal Image")

axs[1].imshow(binary_fence_mask, cmap='gray')
axs[1].axis('off')
axs[1].set_title("Binary Fence Mask")

axs[2].imshow(hough_line_mask, cmap='gray')
axs[2].axis('off')
axs[2].set_title("Hough Line Mask")

axs[3].imshow(dilated_mask, cmap='gray')
axs[3].axis('off')
axs[3].set_title("Dilated Mask")

axs[4].imshow(inpainted_image_final, cmap='gray')
axs[4].axis('off')
axs[4].set_title("Final Inpainted Image")

axs[5].imshow(final_filtered_image, cmap='gray')
axs[5].axis('off')
axs[5].set_title("Post-Processed Image")

axs[6].imshow(clahe_enhanced_image, cmap='gray')
axs[6].axis('off')
axs[6].set_title("CLAHE Enhanced Image")

plt.show()

# Resizing mask

import cv2

# Paths to the original image and the mask
original_image_path = "/content/drive/MyDrive/DATASET/test/mask/FLIR0993_16012.tif"
mask_image_path = "/content/drive/MyDrive/DATASET/test/mask/thinned_mask.png"

# Load the original image and the mask
original_image = cv2.imread(original_image_path, cv2.IMREAD_GRAYSCALE)
mask_image = cv2.imread(mask_image_path, cv2.IMREAD_GRAYSCALE)

# Check dimensions
original_dimensions = original_image.shape
mask_dimensions = mask_image.shape

print(f"Original Image Dimensions: {original_dimensions}")
print(f"Mask Image Dimensions: {mask_dimensions}")

# Resize the mask if dimensions are different
if original_dimensions != mask_dimensions:
    print("Dimensions are different. Resizing the mask...")
    resized_mask = cv2.resize(mask_image, (original_dimensions[1], original_dimensions[0]), interpolation=cv2.INTER_NEAREST)

    # Save the resized mask (optional)
    output_resized_mask_path = "/content/drive/MyDrive/DATASET/test/mask/resized_thinned_mask.png"
    cv2.imwrite(output_resized_mask_path, resized_mask)
    print(f"✅ Mask resized and saved at: {output_resized_mask_path}")
else:
    print("✅ Dimensions match. No resizing needed.")

# Adjust mask colors

from PIL import Image
import numpy as np

# Define file paths
input_mask_path = "/content/drive/MyDrive/IMG_9074.PNG"
output_mask_path = "/content/drive/MyDrive/DATASET/test/mask/processed_fence_mask6.png"

# Load the image and ensure it has an alpha channel (RGBA)
im = Image.open(input_mask_path).convert("RGBA")

# Extract the alpha channel (which represents opacity)
alpha_channel = im.getchannel("A")  # This returns a grayscale image (mode "L")

# Convert the alpha channel to a NumPy array
alpha_np = np.array(alpha_channel)

# Define a threshold: pixels with alpha greater than this are considered part of the fence.
threshold = 128  # Adjust as needed (0-255 scale)
binary_mask_np = (alpha_np > threshold).astype(np.uint8) * 255  # Fence: 255 (white), Background: 0 (black)

# Convert the binary mask back to a PIL Image in mode "L" (grayscale)
binary_mask = Image.fromarray(binary_mask_np, mode="L")

# Resize the mask to the desired dimensions.
# PIL expects size as (width, height), so (556, 417) since your original dimensions are (417, 556).
resized_mask = binary_mask.resize((556, 417), resample=Image.NEAREST)

# Optionally, display the resulting mask
resized_mask.show()

# Save the processed mask to the desired output path.
resized_mask.save(output_mask_path)
print("Processed mask saved to:", output_mask_path)

# Siren Inpainting Code

import torch
from torch import nn
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
import cv2
import os
from PIL import Image

# -----------------------------------------------
# Utility: Create a flattened coordinate grid.
# -----------------------------------------------
def get_mgrid(sidelen, dim=2):
    tensors = tuple(torch.linspace(-1, 1, steps=sidelen[i]) for i in range(dim))
    # Explicit indexing (requires PyTorch 1.10+)
    mgrid = torch.stack(torch.meshgrid(*tensors, indexing='ij'), dim=-1)
    mgrid = mgrid.reshape(-1, dim)
    return mgrid

# -----------------------------------------------
# Sine Layer (SIREN)
# -----------------------------------------------
class SineLayer(nn.Module):
    def __init__(self, in_features, out_features, bias=True,
                 is_first=False, omega_0=30):
        super().__init__()
        self.omega_0 = omega_0
        self.is_first = is_first
        self.in_features = in_features
        self.linear = nn.Linear(in_features, out_features, bias=bias)
        self.init_weights()

    def init_weights(self):
        with torch.no_grad():
            if self.is_first:
                self.linear.weight.uniform_(-1 / self.in_features, 1 / self.in_features)
            else:
                self.linear.weight.uniform_(
                    -np.sqrt(6 / self.in_features) / self.omega_0,
                    np.sqrt(6 / self.in_features) / self.omega_0
                )

    def forward(self, input):
        return torch.sin(self.omega_0 * self.linear(input))

# -----------------------------------------------
# SIREN Model
# -----------------------------------------------
class Siren(nn.Module):
    def __init__(self, in_features, hidden_features, hidden_layers, out_features,
                 outermost_linear=False, first_omega_0=30, hidden_omega_0=30.):
        super().__init__()
        self.net = []
        self.net.append(SineLayer(in_features, hidden_features,
                                  is_first=True, omega_0=first_omega_0))
        for i in range(hidden_layers):
            self.net.append(SineLayer(hidden_features, hidden_features,
                                      is_first=False, omega_0=hidden_omega_0))
        if outermost_linear:
            final_linear = nn.Linear(hidden_features, out_features)
            with torch.no_grad():
                final_linear.weight.uniform_(
                    -np.sqrt(6 / hidden_features) / hidden_omega_0,
                    np.sqrt(6 / hidden_features) / hidden_omega_0
                )
            self.net.append(final_linear)
        else:
            self.net.append(SineLayer(hidden_features, out_features,
                                      is_first=False, omega_0=hidden_omega_0))
        self.net = nn.Sequential(*self.net)

    def forward(self, coords):
        coords = coords.clone().detach().requires_grad_(True)
        output = self.net(coords)
        return output, coords

# -----------------------------------------------
# Dataset for Image Inpainting with Mask Dilution
# -----------------------------------------------
class ImageFittingInpaint(Dataset):
    def __init__(self, image_data, mask_data, sidelength, dilution=0.0):
        """
        image_data: 2D numpy array (grayscale, normalized [0,1])
        mask_data: 2D numpy array (grayscale, normalized [0,1])
                   where known areas are 1 and fence (occluded) areas are 0.
        sidelength: tuple (height, width)
        dilution: small nonzero weight to assign to the occluded areas.
                  Setting dilution=0.0 keeps the fence regions exactly 0.
        """
        super().__init__()
        self.pixels = torch.tensor(image_data).float().view(sidelength[0], sidelength[1], 1)
        # Invert the mask so that known regions become 1 and occluded areas become 0.
        self.mask = torch.tensor(mask_data).float().view(sidelength[0], sidelength[1], 1)
        self.mask = 1.0 - self.mask
        # Apply dilution: with dilution=0.0, the unknown (fence) regions remain exactly 0.
        self.mask = self.mask * (1 - dilution) + dilution
        self.coords = get_mgrid(sidelength, dim=2)

    def __len__(self):
        return 1

    def __getitem__(self, idx):
        if idx > 0:
            raise IndexError("Dataset contains only one image.")
        return self.coords, self.pixels.view(-1, 1), self.mask.view(-1, 1)

# -----------------------------------------------
# Total Variation Loss Function
# -----------------------------------------------
def total_variation_loss(img):
    loss_tv = torch.mean(torch.abs(img[:, :, :-1] - img[:, :, 1:])) + \
              torch.mean(torch.abs(img[:, :-1, :] - img[:, 1:, :]))
    return loss_tv

# -----------------------------------------------
# Gradient Loss Function
# -----------------------------------------------
def gradient_loss(img):
    dx = torch.abs(img[:, :, 1:] - img[:, :, :-1])
    dy = torch.abs(img[:, 1:, :] - img[:, :-1, :])
    return dx.mean() + dy.mean()

# -----------------------------------------------
# Load and Preprocess TIF Image and Mask
# -----------------------------------------------
tif_image_path = "/content/drive/MyDrive/DATASET/train/images/FLIR0993_16012.tif"
mask_path = "/content/drive/MyDrive/DATASET/test/mask/processed_fence_mask6.png"

img = cv2.imread(tif_image_path, cv2.IMREAD_GRAYSCALE)
if img is None:
    raise ValueError("Could not load the TIF image from: " + tif_image_path)
img = img.astype(np.float32) / 255.0

# Load the mask using PIL (since it is a PNG and already in the correct size)
mask_pil = Image.open(mask_path).convert("L")
mask_img = np.array(mask_pil).astype(np.float32) / 255.0
mask_img = np.round(mask_img)  # Ensure binary: known=1, fence=0

print("Mask shape:", mask_img.shape)  # Should match the original image dimensions, e.g., (417, 556)
sidelength = (img.shape[0], img.shape[1])

thermal_image_dataset = ImageFittingInpaint(img, mask_img, sidelength, dilution=0.0)
dataloader = DataLoader(thermal_image_dataset, batch_size=1, pin_memory=True, num_workers=0)


# Initialize the SIREN Model and Optimizer
img_siren = Siren(in_features=2, out_features=1, hidden_features=256, hidden_layers=3, outermost_linear=True)
optimizer = torch.optim.Adam(lr=1e-4, params=img_siren.parameters())

# Create directory for saving reconstructed images
output_recon_path = "/content/drive/MyDrive/DATASET/sirenimages/"
os.makedirs(output_recon_path, exist_ok=True)

# Training parameters
total_steps = 500          # Total training steps (epochs)
steps_til_summary = 10     # Frequency for printing summaries and showing plots
save_interval = 50         # Save reconstructed image every 50 steps

# Base weights for loss components (we removed the L2 loss)
l1_weight = 1.0            # Weight for masked L1 loss
base_tv_weight = 0.001     # Initial weight for TV loss
grad_weight = 0.001        # Weight for gradient loss

model_input, ground_truth, mask_tensor = next(iter(dataloader))


# Training Loop: L1 Loss (masked) + TV Loss + Gradient Loss
for step in range(total_steps):
    model_output, coords = img_siren(model_input)

    # Compute the masked L1 loss to encourage brightness consistency
    l1_loss = (torch.abs(model_output - ground_truth) * mask_tensor).mean()

    # Reshape output for spatial loss computation (batch size 1, H, W)
    recon_img = model_output.view(1, sidelength[0], sidelength[1])

    # Compute Total Variation Loss and Gradient Loss
    tv_loss = total_variation_loss(recon_img)
    grad_loss = gradient_loss(recon_img)

    # Dynamically adjust the TV loss weight after 300 epochs to avoid oversmoothing
    if step < 300:
        current_tv_weight = base_tv_weight
    else:
        current_tv_weight = base_tv_weight * (total_steps - step) / (total_steps - 300)
        current_tv_weight = max(current_tv_weight, 0.0001)

    # Total loss now only uses masked L1 loss, TV loss, and gradient loss.
    loss = l1_weight * l1_loss + current_tv_weight * tv_loss + grad_weight * grad_loss

    if step % steps_til_summary == 0:
        print(f"Step {step}, L1 Loss: {l1_loss.item():.6f}, "
              f"TV Loss: {tv_loss.item():.6f} (weight: {current_tv_weight:.6f}), "
              f"Grad Loss: {grad_loss.item():.6f}, Total Loss: {loss.item():.6f}")
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        recon_img_vis = model_output.cpu().view(sidelength[0], sidelength[1]).detach().numpy()
        true_img = ground_truth.cpu().view(sidelength[0], sidelength[1]).detach().numpy()
        mask_vis = mask_tensor.cpu().view(sidelength[0], sidelength[1]).detach().numpy()
        axes[0].imshow(recon_img_vis, cmap='gray')
        axes[0].set_title("Reconstructed (Inpainted) Image")
        axes[1].imshow(true_img, cmap='gray')
        axes[1].set_title("Ground Truth")
        axes[2].imshow(mask_vis, cmap='gray')
        axes[2].set_title("Effective Mask (Known=1, Fence=0)")
        plt.show()

    if step % save_interval == 0:
        # Save the reconstructed image to the sirenimages folder
        recon_img_to_save = model_output.cpu().view(sidelength[0], sidelength[1]).detach().numpy()
        recon_img_to_save = np.clip(recon_img_to_save, 0, 1) * 255
        recon_img_to_save = recon_img_to_save.astype(np.uint8)
        save_path = os.path.join(output_recon_path, f"reconstructed_fin_{step}.png")
        cv2.imwrite(save_path, recon_img_to_save)
        print(f"Reconstructed image saved: {save_path}")

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Dilute the mask
from PIL import Image
import numpy as np

# Define file paths
input_mask_path = "/content/drive/MyDrive/IMG_9074.PNG"  # Your original mask file with transparency
output_mask_path = "/content/drive/MyDrive/DATASET/test/mask/processed_fence_mask6.png"

# Load the image and ensure it has an alpha channel (RGBA)
im = Image.open(input_mask_path).convert("RGBA")

# Extract the alpha channel (which represents opacity)
alpha_channel = im.getchannel("A")  # This returns a grayscale image (mode "L")

# Convert the alpha channel to a NumPy array
alpha_np = np.array(alpha_channel)

# Define a threshold: pixels with alpha greater than this are considered part of the fence.
threshold = 128  # Adjust as needed (0-255 scale)
binary_mask_np = (alpha_np > threshold).astype(np.uint8) * 255  # Fence: 255 (white), Background: 0 (black)

# Convert the binary mask back to a PIL Image in mode "L" (grayscale)
binary_mask = Image.fromarray(binary_mask_np, mode="L")

# Resize the mask to the desired dimensions.
# PIL expects size as (width, height), so (556, 417) since your original dimensions are (417, 556).
resized_mask = binary_mask.resize((556, 417), resample=Image.NEAREST)

# Optionally, display the resulting mask
resized_mask.show()

# Save the processed mask to the desired output path.
resized_mask.save(output_mask_path)
print("Processed mask saved to:", output_mask_path)

#resize mask
from PIL import Image
import os

# Path to your original mask
mask_path = '/content/drive/MyDrive/DATASET/test/mask/processed_fence_mask6.png'

# Load the mask and convert to grayscale (if not already)
mask_img = Image.open(mask_path).convert('L')

# Define the target size (width, height)
target_size = (416, 416)

# Resize the mask using BICUBIC interpolation
mask_resized = mask_img.resize(target_size, Image.BICUBIC)

# Save the resized mask to a new file
new_mask_path = '/content/drive/MyDrive/DATASET/test/mask/processed_fence_mask6_resized.png'
mask_resized.save(new_mask_path)
print("Resized mask saved to", new_mask_path)

# Convert my TIF images to RGB and save them in a new file for edge connect
import glob
import os

# Path to your converted RGB images
rgb_dir = '/content/drive/MyDrive/DATASET/train/images_rgb'
# Get all TIFF files in that folder (adjust the extension if needed)
image_paths = sorted(glob.glob(os.path.join(rgb_dir, '*.tif')))

# Write these paths to your test_flist.txt file
with open('/content/drive/MyDrive/DATASET/test_flist.txt', 'w') as f:
    for path in image_paths:
        f.write(path + '\n')

print("Test file list updated with", len(image_paths), "entries.")

# resize rgb images
import os
from PIL import Image
import glob

# Define input and output folders
input_folder = '/content/drive/MyDrive/DATASET/train/images_rgb'
output_folder = '/content/drive/MyDrive/DATASET/train/images_resized'
os.makedirs(output_folder, exist_ok=True)

# Set target size: (width, height)
target_size = (562, 423)

# Get all TIFF files in the input folder
image_paths = sorted(glob.glob(os.path.join(input_folder, '*.tif')))

print("Found", len(image_paths), "images. Resizing to", target_size, "and saving to", output_folder)

# Process each image
for filepath in image_paths:
    try:
        with Image.open(filepath) as img:
            # Resize the image using BICUBIC interpolation
            resized_img = img.resize(target_size, Image.BICUBIC)
            # Save the resized image to the output folder with the same filename
            base_name = os.path.basename(filepath)
            new_path = os.path.join(output_folder, base_name)
            resized_img.save(new_path)
            print("Resized and saved:", new_path)
    except Exception as e:
        print("Error processing", filepath, ":", e)

print("Resizing complete.")

# regenerate my test file list for RGB images
import glob
import os

# New directory with converted RGB images
rgb_dir = '/content/drive/MyDrive/DATASET/train/images_rgb'

# Get all TIFF files from the new directory
image_paths = sorted(glob.glob(os.path.join(rgb_dir, '*.tif')))

# Write the file paths to your test file list
with open('/content/drive/MyDrive/DATASET/test_flist.txt', 'w') as f:
    for path in image_paths:
        f.write(path + '\n')

print("Updated test_flist.txt with", len(image_paths), "entries.")

# image shape test should be all (417, 556, 3)
import numpy as np
from imageio import imread
from skimage.color import gray2rgb
import glob

# Test one image from your converted RGB directory:
img = imread('/content/drive/MyDrive/DATASET/train/images_rgb/FLIR0993_15986.tif')
print("Original shape:", img.shape)
if img.ndim == 2:
    img = gray2rgb(img)
elif img.ndim == 3 and img.shape[2] == 1:
    img = np.repeat(img, 3, axis=2)
print("After conversion:", img.shape)

# Create mask file list for each frame
import os

# Path to the test file list (images)
test_flist_path = '/content/drive/MyDrive/DATASET/test_flist.txt'
# Path to your new resized mask
new_mask_path = '/content/drive/MyDrive/DATASET/test/mask/processed_fence_mask6_resized.png'
# Output path for the new mask file list
output_mask_txt = '/content/drive/MyDrive/DATASET/test/mask/processed_fence_mask6_resized.txt'

# Read the test file list to determine the number of images
with open(test_flist_path, 'r') as f:
    lines = f.readlines()
num_images = len(lines)

# Write the new mask file list, repeating the resized mask path for each image
with open(output_mask_txt, 'w') as f:
    for _ in range(num_images):
        f.write(new_mask_path + '\n')

print("Created mask file list:", output_mask_txt, "with", num_images, "entries.")

# Testing everything is correct size
# size comparison of images and mask
from PIL import Image
import numpy as np

# Set paths to one example RGB image and your new mask
rgb_image_path = '/content/drive/MyDrive/DATASET/train/images_resized/FLIR0993_15986.tif'
mask_image_path = '/content/drive/MyDrive/DATASET/test/mask/maskthin_processed.png'

# Open the images using PIL
rgb_img = Image.open(rgb_image_path).convert('RGB')  # Ensure it's in RGB mode
mask_img = Image.open(mask_image_path).convert('L')    # Convert mask to grayscale

# Print out sizes (PIL returns size as (width, height))
print("RGB image size (width, height):", rgb_img.size)
print("Mask image size (width, height):", mask_img.size)

# Optionally, convert to numpy arrays to check the shape in (H, W, C)
rgb_np = np.array(rgb_img)
mask_np = np.array(mask_img)
print("RGB numpy shape (H, W, C):", rgb_np.shape)   # Expected: (height, width, 3)
print("Mask numpy shape (H, W):", mask_np.shape)      # Expected: (height, width)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/EdgeConnect
import sys
sys.path.append('/content/drive/MyDrive/EdgeConnect')

# Commented out IPython magic to ensure Python compatibility.
# #Adjust config file accordingly
# %%writefile /content/drive/MyDrive/EdgeConnect/checkpoints/config.yml
# MODE: 2
# MODEL: 3
# INPUT_SIZE: 0
# TEST_FLIST: /content/drive/MyDrive/DATASET/test_resized_flist.txt
# TEST_MASK_FLIST:  /content/drive/MyDrive/DATASET/test/mask/test1_mask_flist.txt
# TEST_EDGE_FLIST: /content/drive/MyDrive/DATASET/test_resized_flist.txt
# RESULTS: /content/drive/MyDrive/DATASET/results2
# LR: 0.0001
# D2G_LR: 0.1
# BETA1: 0.0
# BETA2: 0.9
# BATCH_SIZE: 8
# SIGMA: 2
# MAX_ITERS: 2000000
# EDGE_THRESHOLD: 0.5
# L1_LOSS_WEIGHT: 1
# FM_LOSS_WEIGHT: 10
# STYLE_LOSS_WEIGHT: 1
# CONTENT_LOSS_WEIGHT: 1
# INPAINT_ADV_LOSS_WEIGHT: 0.01
# GAN_LOSS: nsgan
# GAN_POOL_SIZE: 0
# SAVE_INTERVAL: 1000
# SAMPLE_INTERVAL: 1000
# SAMPLE_SIZE: 12
# EVAL_INTERVAL: 0
# LOG_INTERVAL: 10

#Running EdgeConnect Inpainting on Images
!python main.py --path /content/drive/MyDrive/EdgeConnect/checkpoints --model 3

# Overlap original frame with the inpainted region using masked region
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Define File Paths
orig_path = "/content/drive/MyDrive/DATASET/train/images/FLIR0993_16012.tif"
inpainted_path = "/content/drive/MyDrive/DATASET/sirenimages/reconstructed_step_450.png"
mask_path = "/content/drive/MyDrive/DATASET/test/mask/processed_fence_mask6.png"

# Load the Images
# Load the original image in grayscale
orig = cv2.imread(orig_path, cv2.IMREAD_GRAYSCALE)
if orig is None:
    raise ValueError("Could not load the original image from: " + orig_path)

# Load the inpainted image in grayscale
inpainted = cv2.imread(inpainted_path, cv2.IMREAD_GRAYSCALE)
if inpainted is None:
    raise ValueError("Could not load the inpainted image from: " + inpainted_path)

# Load the mask image in grayscale
mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
if mask is None:
    raise ValueError("Could not load the mask image from: " + mask_path)

# Ensure the Mask is Binary
# Threshold the mask so that pixels above 127 are set to 255 (white)
_, binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)

# Convert the binary mask to a float image in the range [0, 1]
binary_mask_float = binary_mask.astype(np.float32) / 255.0

# Create the Composite Image
# For pixels where the mask is 1 (i.e., fence region), use the inpainted image.
# For pixels where the mask is 0 (i.e., non-fence region), use the original image.
composite = (binary_mask_float * inpainted.astype(np.float32) +
             (1 - binary_mask_float) * orig.astype(np.float32))
composite = np.clip(composite, 0, 255).astype(np.uint8)

# Display the Composite Image
plt.figure(figsize=(10, 10))
plt.imshow(composite, cmap='gray')
plt.title("Composite: Original with Inpainted Regions Overlaid")
plt.axis("off")
plt.show()

# Optionally, Save the Composite Image
output_composite_path = "/content/drive/MyDrive/DATASET/sirenimages/composite_result.png"
cv2.imwrite(output_composite_path, composite)
print("Composite image saved to:", output_composite_path)

#Weighted Median Tracking single point
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt

# User Parameters
video_path = "/content/drive/MyDrive/DATASET/composite_video_18078_18166.mp4"  # New composite video
output_video_path = "/content/drive/MyDrive/DATASET/results3/tracked_face_fixed.mp4"
init_roi = (115, 320, 50, 30)   # Initial (and fixed) ROI starting coordinates and size

lk_params = dict(winSize=(15, 15),
                 maxLevel=2,
                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

# Open Video and Set Up Writer
cap = cv2.VideoCapture(video_path)
if not cap.isOpened():
    raise ValueError("Error opening video file: " + video_path)

frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

# Tracking Variables
tracking_started = False
p0 = None
prev_gray = None
last_center = None  # Tracked center
frame_idx = 0

# Helper Function: Initialize Tracking

def initialize_tracking(frame, roi):
    global p0, prev_gray, last_center
    x, y, w, h = roi
    # Convert frame to grayscale
    first_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    # Create mask for ROI
    roi_mask = np.zeros_like(first_gray)
    roi_mask[y:y+h, x:x+w] = 255
    # Find good features in the ROI
    p0 = cv2.goodFeaturesToTrack(first_gray, mask=roi_mask, maxCorners=20,
                                 qualityLevel=0.01, minDistance=5, blockSize=7)
    if p0 is None:
        print("No features found in ROI.")
        return False
    prev_gray = first_gray.copy()
    last_center = (x + w/2, y + h/2)
    return True

# Main Loop: Process Video Frames
while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Initialize tracking on the very first frame using init_roi.
    if frame_idx == 0:
        tracking_started = initialize_tracking(frame, init_roi)

    if tracking_started and p0 is not None and prev_gray is not None:
        current_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # Calculate optical flow from previous frame to current frame.
        p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, current_gray, p0, None, **lk_params)
        if p1 is not None and st is not None:
            good_new = p1[st == 1]
            if len(good_new) > 0:
                # Compute weighted median of the x and y coordinates.
                # (Alternatively, you could simply use the mean of the good points.)
                weights = []
                for pt in good_new:
                    ix, iy = int(pt[0]), int(pt[1])
                    brightness = current_gray[iy, ix]
                    weights.append(brightness)
                weights = np.array(weights, dtype=np.float32)

                # Compute weighted median for x coordinate
                sort_idx = np.argsort(good_new[:, 0])
                sorted_x = good_new[sort_idx, 0]
                sorted_w = weights[sort_idx]
                cdf = np.cumsum(sorted_w)
                cutoff = cdf[-1] * 0.5
                median_idx = np.searchsorted(cdf, cutoff)
                median_x = sorted_x[median_idx]

                # Compute weighted median for y coordinate
                sort_idx_y = np.argsort(good_new[:, 1])
                sorted_y = good_new[sort_idx_y, 1]
                sorted_w_y = weights[sort_idx_y]
                cdf_y = np.cumsum(sorted_w_y)
                cutoff_y = cdf_y[-1] * 0.5
                median_idx_y = np.searchsorted(cdf_y, cutoff_y)
                median_y = sorted_y[median_idx_y]

                new_center = (median_x, median_y)
                last_center = new_center

                # Draw a bounding box (using the fixed ROI size) centered at the new_center.
                box_x = int(new_center[0] - init_roi[2] / 2)
                box_y = int(new_center[1] - init_roi[3] / 2)
                cv2.rectangle(frame, (box_x, box_y), (box_x + init_roi[2], box_y + init_roi[3]), (0, 255, 0), 2)

                # Update features for next iteration.
                p0 = good_new.reshape(-1, 1, 2)
            prev_gray = current_gray.copy()

    video_writer.write(frame)
    cv2_imshow(frame)
    if cv2.waitKey(1) & 0xFF == 27:
        break
    frame_idx += 1

cap.release()
video_writer.release()
cv2.destroyAllWindows()
print("Tracked video saved to:", output_video_path)

#Weighted Median Lucas Kanade Nasal Tracking multiple different points
import cv2
import numpy as np
import os
from google.colab.patches import cv2_imshow

# User parameters
video_path = "/content/drive/MyDrive/DATASET/sirenimages/ECVidOverlay.mp4"
output_video_path = "/content/drive/MyDrive/DATASET/sirenimages/nose_tracking_weightedEC4.mp4"
start_frame_index = 10
init_roi = (220, 340, 50, 30)    # Initial ROI (x, y, w, h)
new_roi1 = (115, 320, 50, 30)     # First new ROI (e.g., at frame 37)
new_roi2 = (485, 115, 50, 30)     # Second new ROI (nose coordinates x=485, y=115)

lk_params = dict(winSize=(15, 15),
                 maxLevel=2,
                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))
fps_factor = 3
outlier_threshold = 20.0

cap = cv2.VideoCapture(video_path)
if not cap.isOpened():
    raise ValueError("Error opening video file: " + video_path)

total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
orig_fps = cap.get(cv2.CAP_PROP_FPS)
out_fps = orig_fps * fps_factor

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
video_writer = cv2.VideoWriter(output_video_path, fourcc, out_fps, (frame_width, frame_height))

print(f"Total frames: {total_frames}, Original FPS: {orig_fps:.2f}, Output FPS: {out_fps:.2f}")

tracking_started = False
p0 = None
prev_gray = None
last_center = None  # Store the last computed center
roi_x, roi_y, roi_w, roi_h = init_roi  # Start with initial ROI

frame_idx = 0

# Helper function: weighted median for 1D arrays.
def weighted_median(values, weights):
    sort_idx = np.argsort(values)
    sorted_vals = values[sort_idx]
    sorted_wts = weights[sort_idx]
    cdf = np.cumsum(sorted_wts)
    cutoff = cdf[-1] * 0.5
    median_idx = np.searchsorted(cdf, cutoff)
    return sorted_vals[median_idx]

def initialize_tracking(frame, roi):
    """Initialize tracking using a new ROI."""
    global p0, prev_gray, last_center, tracking_started, roi_x, roi_y, roi_w, roi_h
    roi_x, roi_y, roi_w, roi_h = roi  # Update ROI to new position
    print(f"Initializing tracking at frame {frame_idx} using ROI {roi}...")

    first_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    roi_mask = np.zeros_like(first_gray)
    roi_mask[roi_y:roi_y+roi_h, roi_x:roi_x+roi_w] = 255
    p0 = cv2.goodFeaturesToTrack(first_gray, mask=roi_mask, maxCorners=20,
                                 qualityLevel=0.01, minDistance=5, blockSize=7)
    if p0 is None:
        print("Warning: No features found in ROI. Adjust ROI position.")
        return False  # Tracking not started

    prev_gray = first_gray.copy()
    last_center = (roi_x + roi_w/2, roi_y + roi_h/2)
    return True  # Tracking started

while True:
    ret, frame = cap.read()
    if not ret:
        break

    if frame_idx < start_frame_index:
        video_writer.write(frame)
        frame_idx += 1
        continue

    # Initialize tracking at start_frame_index with the initial ROI
    if not tracking_started and frame_idx == start_frame_index:
        tracking_started = initialize_tracking(frame, init_roi)

    # First change: reinitialize tracking at frame 37 using new_roi1
    if frame_idx == 37:
        print("Switching tracking to new position at frame 37")
        tracking_started = initialize_tracking(frame, new_roi1)

    # Second change: reinitialize tracking at frame 124 using new_roi2
    if frame_idx == 124:
        print("Switching tracking to new position at frame 124")
        tracking_started = initialize_tracking(frame, new_roi2)

    if tracking_started and p0 is not None and prev_gray is not None:
        current_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, current_gray, p0, None, **lk_params)
        if p1 is not None and st is not None:
            good_new = p1[st == 1]
            if len(good_new) > 0:
                # Compute brightness weights for each point
                weights = []
                for pt in good_new:
                    ix, iy = int(pt[0]), int(pt[1])
                    brightness = current_gray[iy, ix]
                    weights.append(brightness)
                weights = np.array(weights, dtype=np.float32)
                median_x = weighted_median(good_new[:, 0], weights)
                median_y = weighted_median(good_new[:, 1], weights)
                new_center = (median_x, median_y)

                # Update last_center and draw fixed-size bounding box centered at new_center.
                last_center = new_center
                box_x = int(new_center[0] - roi_w/2)
                box_y = int(new_center[1] - roi_h/2)
                cv2.rectangle(frame, (box_x, box_y), (box_x+roi_w, box_y+roi_h), (0, 255, 0), 2)

                p0 = good_new.reshape(-1, 1, 2)
            prev_gray = current_gray.copy()

    video_writer.write(frame)
    cv2_imshow(frame)
    if cv2.waitKey(1) & 0xFF == 27:
        break
    frame_idx += 1

cap.release()
video_writer.release()
cv2.destroyAllWindows()
print("Weighted median tracking video saved to:", output_video_path)

# Thermal Readings Extracted from Tracking Video
import cv2
import numpy as np
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt


video_path = "/content/drive/MyDrive/DATASET/composite_video_18078_18166.mp4"  # Input composite video
output_video_path = "/content/drive/MyDrive/DATASET/tracked_face_fixed_with_temp.mp4"
init_roi = (115, 320, 50, 30)   # Fixed ROI starting position

lk_params = dict(winSize=(15, 15),
                 maxLevel=2,
                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))


cap = cv2.VideoCapture(video_path)
if not cap.isOpened():
    raise ValueError("Error opening video file: " + video_path)

frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

# Tracking Variables
tracking_started = False
p0 = None
prev_gray = None
last_center = None  # This will hold the (x, y) coordinate of the tracked nose
frame_idx = 0

# List to store temperature readings as (frame_idx, temperature)
temperature_over_time = []

# Helper Function: Initialize Tracking
def initialize_tracking(frame, roi):
    global p0, prev_gray, last_center
    x, y, w, h = roi
    first_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    roi_mask = np.zeros_like(first_gray)
    roi_mask[y:y+h, x:x+w] = 255
    p0 = cv2.goodFeaturesToTrack(first_gray, mask=roi_mask, maxCorners=20,
                                 qualityLevel=0.01, minDistance=5, blockSize=7)
    if p0 is None:
        print("Warning: No features found in ROI.")
        return False
    prev_gray = first_gray.copy()
    last_center = (x + w/2, y + h/2)
    return True

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Initialize tracking on the very first frame using init_roi.
    if frame_idx == 0:
        tracking_started = initialize_tracking(frame, init_roi)

    if tracking_started and p0 is not None and prev_gray is not None:
        current_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, current_gray, p0, None, **lk_params)
        if p1 is not None and st is not None:
            good_new = p1[st == 1]
            if len(good_new) > 0:
                # Use the average (or weighted median) of good feature positions as the new center.
                # Here, we simply use the mean.
                new_center = np.mean(good_new, axis=0)
                last_center = (new_center[0], new_center[1])

                # Draw the fixed-size bounding box around the tracked center.
                box_x = int(last_center[0] - init_roi[2] / 2)
                box_y = int(last_center[1] - init_roi[3] / 2)
                cv2.rectangle(frame, (box_x, box_y), (box_x + init_roi[2], box_y + init_roi[3]), (0, 255, 0), 2)

                p0 = good_new.reshape(-1, 1, 2)
            prev_gray = current_gray.copy()


        # Temperature Extraction at Tracked Nose

        # Define a small window around the tracked nose center.
        cx, cy = int(last_center[0]), int(last_center[1])
        window_size = 3  # e.g., 3x3 window
        half_window = window_size // 2
        # Ensure the window is within image bounds.
        x1 = max(cx - half_window, 0)
        y1 = max(cy - half_window, 0)
        x2 = min(cx + half_window + 1, current_gray.shape[1])
        y2 = min(cy + half_window + 1, current_gray.shape[0])
        window = current_gray[y1:y2, x1:x2]
        # Compute average pixel value as temperature proxy.
        avg_temp = np.mean(window)
        temperature_over_time.append((frame_idx, avg_temp))

    video_writer.write(frame)
    cv2_imshow(frame)
    if cv2.waitKey(1) & 0xFF == 27:
        break
    frame_idx += 1

cap.release()
video_writer.release()
cv2.destroyAllWindows()
print("Tracked video saved to:", output_video_path)


if temperature_over_time:
    frames, temps = zip(*temperature_over_time)
    plt.figure(figsize=(10, 5))
    plt.plot(frames, temps, marker='o', linestyle='-')
    plt.xlabel("Frame Number")
    plt.ylabel("Temperature Proxy (Avg Pixel Value)")
    plt.title("Thermal Readings at Tracked Nose Over Frames")
    plt.grid(True)
    plt.show()
else:
    print("No temperature data extracted.")

#PSNR and SSIM Results
import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.metrics import structural_similarity as ssim
import math


# File Paths (update as needed)

# Ground truth image (with the artificial fence manually added)
ground_truth_path = "/content/drive/MyDrive/DATASET/train/images/FLIR0993_18145.tif"
# Inpainted output from EdgeConnect
edgeconnect_path = "/content/drive/MyDrive/DATASET/inpaintingmeasureimages/FLIR0993_18145.tif"
# Inpainted output from SIREN
siren_path = "/content/drive/MyDrive/DATASET/inpaintingmeasureimages/FLIR0993_18145_final_reconstructed.png"

# Load Images in Grayscale
gt_img = cv2.imread(ground_truth_path, cv2.IMREAD_GRAYSCALE)
ec_img = cv2.imread(edgeconnect_path, cv2.IMREAD_GRAYSCALE)
siren_img = cv2.imread(siren_path, cv2.IMREAD_GRAYSCALE)

if gt_img is None or ec_img is None or siren_img is None:
    raise ValueError("One or more images could not be loaded. Check the file paths.")

# Optionally, check dimensions and resize if needed so they match.
# For this example, we assume the dimensions are already consistent.


# Define the Nose Region Bounding Box

# Manually specify the region that covers only the nose.
# (x, y, width, height) - adjust these values to fit your nose region.
nose_bbox = (494, 95, 37, 40)  # Example values; update these as needed.
x, y, w, h = nose_bbox
print(f"Using nose bounding box: x={x}, y={y}, w={w}, h={h}")


# Crop the Images to the Nose Region

gt_crop = gt_img[y:y+h, x:x+w]
ec_crop = ec_img[y:y+h, x:x+w]
siren_crop = siren_img[y:y+h, x:x+w]


# Function to Compute PSNR

def compute_psnr(img1, img2, max_val=255.0):
    mse = np.mean((img1.astype(np.float64) - img2.astype(np.float64)) ** 2)
    if mse == 0:
        return float('inf')
    return 10 * math.log10((max_val ** 2) / mse)


# Compute Metrics for the Cropped Region

psnr_ec = compute_psnr(gt_crop, ec_crop)
psnr_siren = compute_psnr(gt_crop, siren_crop)

ssim_ec, ssim_map_ec = ssim(gt_crop, ec_crop, full=True)
ssim_siren, ssim_map_siren = ssim(gt_crop, siren_crop, full=True)

print("EdgeConnect vs. Ground Truth (Nose Region):")
print("PSNR: {:.2f} dB".format(psnr_ec))
print("SSIM: {:.4f}".format(ssim_ec))

print("\nSIREN vs. Ground Truth (Nose Region):")
print("PSNR: {:.2f} dB".format(psnr_siren))
print("SSIM: {:.4f}".format(ssim_siren))

# Display the SSIM Maps
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(ssim_map_ec, cmap='viridis')
plt.title("SSIM Map for EdgeConnect (Nose)")
plt.colorbar()
plt.subplot(1, 2, 2)
plt.imshow(ssim_map_siren, cmap='viridis')
plt.title("SSIM Map for SIREN (Nose)")
plt.colorbar()
plt.show()

# Display the cropped regions.
plt.figure(figsize=(15,5))
plt.subplot(1, 3, 1)
plt.imshow(gt_crop, cmap='gray')
plt.title("Ground Truth (Nose Region)")
plt.axis("off")
plt.subplot(1, 3, 2)
plt.imshow(ec_crop, cmap='gray')
plt.title("EdgeConnect (Nose Region)")
plt.axis("off")
plt.subplot(1, 3, 3)
plt.imshow(siren_crop, cmap='gray')
plt.title("SIREN (Nose Region)")
plt.axis("off")
plt.show()

#CSNR calculation
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Paths and Parameters
video_path = "/content/drive/MyDrive/DATASET/composite_video_18078_18166.mp4"  # Update to your EdgeConnect inpainted video
mask_path = "/content/drive/MyDrive/DATASET/test/mask/processed_fence_mask6.png"  # Artificial fence mask
fps_factor = 3


# Load and Process the Mask
mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
if mask is None:
    raise ValueError("Could not load mask from: " + mask_path)
# Convert mask to binary: threshold at 127 to yield values 0 or 1.
_, mask_bin = cv2.threshold(mask, 127, 1, cv2.THRESH_BINARY)
print("Mask dimensions (HxW):", mask_bin.shape)

# Open Video and Initialize Variables
cap = cv2.VideoCapture(video_path)
if not cap.isOpened():
    raise ValueError("Error opening video file: " + video_path)

csnr_values = []
frame_numbers = []
frame_idx = 0

# Process Video Frames
while True:
    ret, frame = cap.read()
    if not ret:
        break
    # Convert frame to grayscale.
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Resize mask if needed to match frame dimensions.
    if gray.shape != mask_bin.shape:
        mask_resized = cv2.resize(mask_bin, (gray.shape[1], gray.shape[0]), interpolation=cv2.INTER_NEAREST)
    else:
        mask_resized = mask_bin

    # Extract pixels in the inpainted region (mask == 1) and background (mask == 0).
    inpaint_region = gray[mask_resized == 1]
    background_region = gray[mask_resized == 0]

    if background_region.size == 0:
        print("Warning: Background region is empty in frame", frame_idx)
        csnr = 0
    else:
        mu_inpaint = np.mean(inpaint_region)
        mu_background = np.mean(background_region)
        sigma_background = np.std(background_region)
        # Avoid division by zero.
        if sigma_background == 0:
            csnr = 0
        else:
            csnr = abs(mu_inpaint - mu_background) / sigma_background

    csnr_values.append(csnr)
    frame_numbers.append(frame_idx)
    frame_idx += 1

cap.release()


# Plot CSNR Over Frames
plt.figure(figsize=(10, 6))
plt.plot(frame_numbers, csnr_values, marker='o', linestyle='-')
plt.xlabel("Frame Number")
plt.ylabel("CSNR")
plt.title("CSNR Tracking Score on EdgeConnect Inpainted Video")
plt.grid(True)
plt.show()